{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "f = open('surname10.txt', 'r')\n",
    "surname_list = f.read().split('\\n')\n",
    "f = open('cityname.txt', 'r')\n",
    "cityname_list = f.read().split('\\n')\n",
    "f = open('frequentword.txt', 'r')\n",
    "frequent_list = f.read().split('\\n')\n",
    "f = open('country.txt', 'r')\n",
    "country_list =f.read().split('\\n')\n",
    "# f = open('blacklist.txt', 'r')\n",
    "# blacklist =f.read().split('\\n')\n",
    "\n",
    "#print surname_list\n",
    "data=pd.read_csv('data.csv', delimiter=',')\n",
    "data['preWord'].fillna('null', inplace=True)\n",
    "data['postWord'].fillna('null', inplace=True)\n",
    "\n",
    "data['isCommon']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data['isCity']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in cityname_list)))\n",
    "data['isFrequentword']=data['word'].apply(lambda s:int(any(x.lower() in s.lower().split() for x in frequent_list)))\n",
    "# data['isStopwords']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in stopwords_list)))\n",
    "data['isCountry']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in country_list)))\n",
    "# data['isBlacked']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in blacklist)))\n",
    "\n",
    "data['wordlen']=data['endPos']-data['startPos']\n",
    "data['isCap']=data['word'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['preisCap']=data['preWord'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['postisCap']=data['postWord'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['preisCommon']=data['preWord'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data['postisCommon']=data['postWord'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data = data[(data['bag'] != 4)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['isPartial'] = 0\n",
    "# print data.head(100)\n",
    "datanew = data[(data['isCap']==1)]\n",
    "datanew_append = data[(data['isCap'] == 0) & (data['label'] == 1)]\n",
    "datanew = datanew.append(datanew_append)\n",
    "\n",
    "# set partial word\n",
    "# datanew_pos = datanew[datanew['label'] == 1]\n",
    "# for i,row in datanew.iterrows():\n",
    "#     flag = 0\n",
    "#     datacmp = datanew_pos[datanew['docID'] == row['docID']]\n",
    "#     for j, row1 in datacmp.iterrows():\n",
    "#         if len(row['word'].strip()) != len(row1['word'].strip()) and row['word'].strip() in row1['word'].strip() and row['label'] == 0 and row['docID'] == row1['docID']:\n",
    "# #             print str(row['docID']) + \" / \" + row['word'] + \" / \" +  row1['word'] + \" / \" + str(row1['docID'])\n",
    "#             flag = 1\n",
    "#             break\n",
    "#     if flag == 1:\n",
    "# #         datanew.set_value(i, 'label', 1)\n",
    "#         datanew.set_value(i, 'isPartial', 1)\n",
    "# datanew['isPartial'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# datanew_rmp = datanew[(datanew['isPartial'] == 0) & (datanew['isFrequentword'] == 0)]\n",
    "# train = datanew_rmp[(datanew_rmp['docID']<=200)]\n",
    "\n",
    "# datanew_rmp = datanew[(datanew['isFrequentword'] == 0)]\n",
    "# train = datanew[(datanew['docID']>=0) & (datanew['docID']<=300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "datanew_rmp = datanew[(datanew['isFrequentword'] == 0)]\n",
    "train = datanew_rmp[(datanew_rmp['docID']>=0) & (datanew_rmp['docID']<=200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['isPartial'] = 0\n",
    "# # print data.head(100)\n",
    "# datanew = data[(data['isCap']==1)]\n",
    "# datanew_append = data[(data['isCap'] == 0) & (data['label'] == 1)]\n",
    "# datanew = datanew.append(datanew_append)\n",
    "# # data_doubletk = datanew[datanew['bag'] == 2]\n",
    "# for i,row in datanew.iterrows():\n",
    "#     if row['bag'] == 1 and (row['postWord'][0].isupper() or row['preWord'][0].isupper()):\n",
    "#         datanew.set_value(i, 'isPartial', 1)\n",
    "        \n",
    "# datanew['isPartial'].fillna(0, inplace=True)\n",
    "# # print datanew[datanew['isPartial'] == 1].shape\n",
    "# # print datanew[datanew['isFrequentword'] == 1].shape\n",
    "# # print datanew[datanew['isFrequentword'] == 0].shape\n",
    "# # datanew_rmp = datanew[(datanew['isFrequentword'] == 0)]\n",
    "# # train = datanew[(datanew['docID']>=0) & (datanew['docID']<=300)]\n",
    "\n",
    "# datanew[(datanew['isPartial'] == 1) & (datanew['label'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "# x=train[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon']]\n",
    "# y=train[['label']]\n",
    "\n",
    "# xt = pd.DataFrame(X_test)\n",
    "# xt['prob'] = p\n",
    "# xt\n",
    "train['random'] = [random.sample([0,1,2,3,4],1)[0] for i in range(train.shape[0])]\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# skf.get_n_splits(x, y)\n",
    "# y = np.array(y.ravel()).astype(int)\n",
    "precision = 0\n",
    "recall = 0\n",
    "for num in [0,1,2,3,4]:\n",
    "    X_train = train[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon','isCity','isCountry', 'docID']][train['random'] != num]\n",
    "    y_train = train['label'][train['random'] != num]\n",
    "    X_test = train[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon','isCity', 'isCountry', 'docID']][train['random'] == num]\n",
    "    y_test = train['label'][train['random'] == num]\n",
    "    word = train['word'][train['random'] == num]\n",
    "    label = train['label'][train['random'] == num]\n",
    "#     X_train, X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     clf = LinearRegression()\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     predsLM=np.where(clf.predict(X_test)>0.34,1,0)\n",
    "#     precision+=precision_score(y_test, predsLM)\n",
    "#     recall+=recall_score(y_test, predsLM)\n",
    "\n",
    "#     clf=LogisticRegression()\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     probLG=clf.predict_proba(X_test)\n",
    "#     predsLG=np.where(probLG[:,1]>0.3,1,0)\n",
    "#     precision+=precision_score(y_test, predsLG)\n",
    "#     recall+=recall_score(y_test, predsLG)\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=200)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     probRF = clf.predict_proba(X_test)\n",
    "#     predsRF = np.where(probRF[:, 1] > 0.4, 1, 0)\n",
    "#     p = [i[1] for i in probRF]\n",
    "#     X_test['pred'] = np.array(p)\n",
    "#     X_test['word'] = word\n",
    "#     X_test['label'] = label\n",
    "#     precision += precision_score(y_test, predsRF)\n",
    "#     recall += recall_score(y_test, predsRF)\n",
    "    \n",
    "#     clf = tree.DecisionTreeClassifier()\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     probDT=clf.predict_proba(X_test)\n",
    "#     predsDT=np.where(probDT[:,1]>0.7,1,0)\n",
    "#     #predsDT\n",
    "#     precision+=precision_score(y_test, predsDT)\n",
    "#     recall+=recall_score(y_test, predsDT)\n",
    "    \n",
    "    clf=svm.SVC(kernel = 'linear', probability=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    probSV=clf.predict_proba(X_test)\n",
    "    predsSV=np.where(probSV[:,1]>0.2,1,0)\n",
    "    precision+=precision_score(y_test, predsSV)\n",
    "    recall+=recall_score(y_test, predsSV)\n",
    "    \n",
    "precision = precision / 5\n",
    "recall = recall / 5\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "FP = X_test[['word']][(X_test['pred'] > 0.388) & (X_test['label'] == 0)]\n",
    "# print FP.shape\n",
    "# FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(FP.to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.60790482055863604, 0.62249738357177775, 0.61511456820880717)\n"
     ]
    }
   ],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
