{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "f = open('surname10.txt', 'r')\n",
    "surname_list = f.read().split('\\n')\n",
    "f = open('cityname.txt', 'r')\n",
    "cityname_list = f.read().split('\\n')\n",
    "f = open('frequentword.txt', 'r')\n",
    "frequent_list = f.read().split('\\n')\n",
    "f = open('country.txt', 'r')\n",
    "country_list =f.read().split('\\n')\n",
    "# f = open('blacklist.txt', 'r')\n",
    "# blacklist =f.read().split('\\n')\n",
    "\n",
    "#print surname_list\n",
    "data=pd.read_csv('data_shuffled.csv', delimiter=',')\n",
    "data['preWord'].fillna('null', inplace=True)\n",
    "data['postWord'].fillna('null', inplace=True)\n",
    "\n",
    "data['isCommon']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data['isCity']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in cityname_list)))\n",
    "data['isFrequentword']=data['word'].apply(lambda s:int(any(x.lower() in s.lower().split() for x in frequent_list)))\n",
    "# data['isStopwords']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in stopwords_list)))\n",
    "data['isCountry']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in country_list)))\n",
    "# data['isBlacked']=data['word'].apply(lambda s:int(any(x.lower() in s.lower() for x in blacklist)))\n",
    "\n",
    "data['wordlen']=data['endPos']-data['startPos']\n",
    "data['isCap']=data['word'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['preisCap']=data['preWord'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['postisCap']=data['postWord'].apply(lambda s: int(all(x[0].isupper() for x in s.split())))\n",
    "data['preisCommon']=data['preWord'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data['postisCommon']=data['postWord'].apply(lambda s:int(any(x.lower() in s.lower() for x in surname_list)))\n",
    "data['preisFrequentword']=data['preWord'].apply(lambda s:int(any(x.lower() in s.lower().split() for x in frequent_list)))\n",
    "data['postisFrequentword']=data['postWord'].apply(lambda s:int(any(x.lower() in s.lower().split() for x in frequent_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['bag'] != 4)]\n",
    "print data[(data['docID'] <= 200)].shape\n",
    "print data[(data['docID'] > 200) & (data['docID'] <= 300)].shape\n",
    "datanew = data[(data['isCap']==1)]\n",
    "datanew_append = data[(data['isCap'] == 0) & (data['label'] == 1)]\n",
    "datanew = datanew.append(datanew_append)\n",
    "print datanew[(datanew['docID'] <= 200)].shape\n",
    "print datanew[(datanew['docID'] > 200) & (datanew['docID'] <= 300)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print data.head(100)\n",
    "# datanew = data[(data['isCap']==1)]\n",
    "# datanew_append = data[(data['isCap'] == 0) & (data['label'] == 1)]\n",
    "# datanew = datanew.append(datanew_append)\n",
    "\n",
    "# set partial word\n",
    "datanew['isPartial'] = 0\n",
    "# datanew_pos = datanew[datanew['label'] == 1]\n",
    "for i,row in datanew.iterrows():\n",
    "    flag = 0\n",
    "    datacmp = datanew[(datanew['docID'] == row['docID']) & (datanew['label'] == 1)]\n",
    "    for j, row1 in datacmp.iterrows():\n",
    "        if len(row['word'].strip()) != len(row1['word'].strip()) and row['word'].strip() in row1['word'].strip() and row['label'] == 0 and row['docID'] == row1['docID']:\n",
    "#             print str(row['docID']) + \" / \" + row['word'] + \" / \" +  row1['word'] + \" / \" + str(row1['docID'])\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "#         datanew.set_value(i, 'label', 1)\n",
    "        datanew.set_value(i, 'isPartial', 1)\n",
    "datanew['isPartial'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# datanew_rmp = datanew[(datanew['isPartial'] == 0) & (datanew['isFrequentword'] == 0)]\n",
    "# train = datanew_rmp[(datanew_rmp['docID']<=200)]\n",
    "\n",
    "# datanew_rmp = datanew[(datanew['isFrequentword'] == 0)]\n",
    "# train = datanew[(datanew['docID']>=0) & (datanew['docID']<=300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5427, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# datanew_rmp = datanew[(datanew['isFrequentword'] == 0) & (datanew['isPartial'] == 0)]\n",
    "# print datanew_rmp[(datanew_rmp['docID'] <= 200)].shape\n",
    "# print datanew_rmp[(datanew_rmp['docID'] > 200) & (datanew['docID'] <= 300)].shape\n",
    "datanew_rmp = datanew\n",
    "train = datanew_rmp[(datanew_rmp['docID'] <= 200) ]\n",
    "test = datanew_rmp[(datanew_rmp['docID']<=300) & (datanew_rmp['docID']>200)]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datanew_rmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "datanew[(datanew['docID']<=200) & (datanew['isFrequentword'] == 0) & (datanew['isPartial'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.41505791505791506, 0.37068965517241381, 0.39162112932604737)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "# x=train[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon']]\n",
    "# y=train[['label']]\n",
    "\n",
    "# xt = pd.DataFrame(X_test)\n",
    "# xt['prob'] = p\n",
    "# xt\n",
    "# train['random'] = [random.sample([0,1,2],1)[0] for i in range(train.shape[0])]\n",
    "# skf = StratifiedKFold(n_splits=10)\n",
    "# skf.get_n_splits(x, y)\n",
    "# y = np.array(y.ravel()).astype(int)\n",
    "precision = 0\n",
    "recall = 0\n",
    "# for num in [0,1,2]:\n",
    "X_train = train[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon','isCity','isCountry', 'docID','isFrequentword', 'preisFrequentword', 'postisFrequentword']]\n",
    "y_train = train['label']\n",
    "X_test = test[['isCommon','wordlen','startPos','bag','preisCap','postisCap','preisCommon','postisCommon','isCity', 'isCountry', 'docID', 'isFrequentword', 'preisFrequentword', 'postisFrequentword']]\n",
    "y_test = test['label']\n",
    "word = test['word']\n",
    "label = test['label']\n",
    "#     X_train, X_test = x.iloc[train_index,:], x.iloc[test_index,:]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "# clf = LinearRegression()\n",
    "# clf.fit(X_train, y_train)\n",
    "# predsLM=np.where(clf.predict(X_test)>0.34,1,0)\n",
    "# precision+=precision_score(y_test, predsLM)\n",
    "# recall+=recall_score(y_test, predsLM)\n",
    "\n",
    "# clf=LogisticRegression()\n",
    "# clf.fit(X_train, y_train)\n",
    "# probLG=clf.predict_proba(X_test)\n",
    "# predsLG=np.where(probLG[:,1]>0.3,1,0)\n",
    "# precision+=precision_score(y_test, predsLG)\n",
    "# recall+=recall_score(y_test, predsLG)\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=200)\n",
    "# clf.fit(X_train, y_train)\n",
    "# #     print clf\n",
    "# probRF = clf.predict_proba(X_test)\n",
    "# predsRF = np.where(probRF[:, 1] > 0.31, 1, 0)\n",
    "# p = [i[1] for i in probRF]\n",
    "# X_test['pred'] = np.array(p)\n",
    "# X_test['word'] = word\n",
    "# X_test['label'] = label\n",
    "# precision += precision_score(y_test, predsRF)\n",
    "# recall += recall_score(y_test, predsRF)\n",
    "    \n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "# probDT=clf.predict_proba(X_test)\n",
    "# predsDT=np.where(probDT[:,1]>0.7,1,0)\n",
    "# #predsDT\n",
    "# precision+=precision_score(y_test, predsDT)\n",
    "# recall+=recall_score(y_test, predsDT)\n",
    "    \n",
    "#     clf=svm.SVC(kernel = 'linear', probability=True)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     probSV=clf.predict_proba(X_test)\n",
    "#     predsSV=np.where(probSV[:,1]>0.3,1,0)\n",
    "#     precision+=precision_score(y_test, predsSV)\n",
    "#     recall+=recall_score(y_test, predsSV)\n",
    "    \n",
    "precision = precision\n",
    "recall = recall\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "FP = X_test[['word','docID']][(X_test['pred'] > 0.4) & (X_test['label'] == 0)]\n",
    "# print FP.shape\n",
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(FP.to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
