{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching (EM) about Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This IPython notebook shows a basic workflow two tables using *py_entitymatching*. We want to match data science books in library of UW-Madison and UIUC.  The book information of UW-Madison is from [here](https://search.library.wisc.edu/search/system?q=Data+Science) and the book information of UIUC is from [here](https://vufind.carli.illinois.edu/vf-uiu/Search/Home?lookfor=Data+Science+&type=all&start_over=1&submit=Find&search=new). Details can be found from our Stage 2 Report [here](https://github.com/iphyer/CS839ClassProject/blob/master/stage2/Stage2Report.pdf). \n",
    "\n",
    "\n",
    "First, we need to import *py_entitymatching* package and other libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import py_entitymatching as em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input tables\n",
    "\n",
    "We begin by loading the input tables.\n",
    "\n",
    "We name the table about UW-Madison `TableA.csv` and the table about UIUC `TableB.csv`. And there are \n",
    "\n",
    "* 4824 tuples in table `TableA.csv`\n",
    "* 5060 tuples in table `TableB.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "table_A = em.read_csv_metadata('../data/TableA.csv', key = 'ID')\n",
    "table_B = em.read_csv_metadata('../data/TableB.csv', key = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4824, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5060, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down sampling\n",
    "Down sampling table A and Bï¼Œ get 1000 examples from both table A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = em.down_sample(table_A, table_B, size=1000, y_param = 1, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table shows the corresponding attributes along with their respective types.\n",
      "Please confirm that the information  has been correctly inferred.\n",
      "If you would like to skip this validation process in the future,\n",
      "please set the flag validate_inferred_attr_types equal to false.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left Attribute</th>\n",
       "      <th>Right Attribute</th>\n",
       "      <th>Left Attribute Type</th>\n",
       "      <th>Right Attribute Type</th>\n",
       "      <th>Example Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Levenshtein Distance; Levenshtein Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author</td>\n",
       "      <td>Author</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Publication</td>\n",
       "      <td>Publication</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Format</td>\n",
       "      <td>Format</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ISBN</td>\n",
       "      <td>ISBN</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>Exact Match; Absolute Norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Series</td>\n",
       "      <td>Series</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical Details</td>\n",
       "      <td>Physical Details</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Left Attribute   Right Attribute                  Left Attribute Type  \\\n",
       "0                ID                ID                short string (1 word)   \n",
       "1             Title             Title                short string (1 word)   \n",
       "2            Author            Author  medium string (5 words to 10 words)   \n",
       "3       Publication       Publication  medium string (5 words to 10 words)   \n",
       "4            Format            Format     short string (1 word to 5 words)   \n",
       "5              ISBN              ISBN                              numeric   \n",
       "6            Series            Series  medium string (5 words to 10 words)   \n",
       "7  Physical Details  Physical Details                short string (1 word)   \n",
       "\n",
       "                  Right Attribute Type  \\\n",
       "0                short string (1 word)   \n",
       "1                short string (1 word)   \n",
       "2     short string (1 word to 5 words)   \n",
       "3     short string (1 word to 5 words)   \n",
       "4     short string (1 word to 5 words)   \n",
       "5                              numeric   \n",
       "6  medium string (5 words to 10 words)   \n",
       "7     short string (1 word to 5 words)   \n",
       "\n",
       "                                                                              Example Features  \n",
       "0                                                 Levenshtein Distance; Levenshtein Similarity  \n",
       "1  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "2                                                           Not Applicable: Types do not match  \n",
       "3                                                           Not Applicable: Types do not match  \n",
       "4  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "5                                                                   Exact Match; Absolute Norm  \n",
       "6  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "7                                                           Not Applicable: Types do not match  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to proceed? (y/n):y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_f = em.get_features_for_blocking(A, B)\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "r = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Title\"]), dlm_dc0(rtuple[\"Title\"]))', block_t, block_s)\n",
    "em.add_feature(block_f, 'Title_Title_jac_dlm_dc0_dlm_dc0', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block tables to get candidate set\n",
    "\n",
    "Here we will use several blockers to remove obviously non-matching tuple pairs from the input tables.\n",
    "\n",
    "For the same book, since we got the data from two different library websites, their attributes may not be the exact same. Therefore, we applied an OverlapBlocker over some of the attributes, including the *Title* and *Author*.\n",
    "\n",
    "After multiple tests, we found the best overlap_size for each attribute - for *Author* and *Title*, we set the overlap_size to be 2 and 4 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "ob = em.OverlapBlocker()\n",
    "C = ob.block_tables(A, B, 'Author', 'Author', \n",
    "                    l_output_attrs=['Title','Author','Publication','Format','ISBN','Series', 'Physical Details'], \n",
    "                    r_output_attrs=['Title','Author','Publication','Format','ISBN','Series', 'Physical Details'], \n",
    "                    overlap_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "D = ob.block_candset(C, 'Title', 'Title', overlap_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D)\n",
    "D.to_csv('Set_C.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from D\n",
    "Sample 300 examples from D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = em.sample_table(D, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create label\n",
    "After manually labeling the data, We get 300 candidates with labels in label_S. <br/>\n",
    "Also, need to set the metadata for label_S appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_S = pd.read_csv('./Set_G.csv')\n",
    "# em.copy_properties(S, label_S)\n",
    "em.set_property(label_S, 'key', '_id')\n",
    "em.set_property(label_S, 'fk_ltable', 'ltable_ID')\n",
    "em.set_property(label_S, 'fk_rtable', 'rtable_ID')\n",
    "label_S_rtable = em.read_csv_metadata('./label_S_rtable.csv')\n",
    "label_S_ltable = em.read_csv_metadata('./label_S_ltable.csv')\n",
    "em.set_property(label_S, 'rtable', label_S_rtable)\n",
    "em.set_property(label_S, 'ltable', label_S_ltable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IJ = em.split_train_test(label_S, train_proportion=0.66, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "I.to_csv('Set_I.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "J.to_csv('Set_J.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table shows the corresponding attributes along with their respective types.\n",
      "Please confirm that the information  has been correctly inferred.\n",
      "If you would like to skip this validation process in the future,\n",
      "please set the flag validate_inferred_attr_types equal to false.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left Attribute</th>\n",
       "      <th>Right Attribute</th>\n",
       "      <th>Left Attribute Type</th>\n",
       "      <th>Right Attribute Type</th>\n",
       "      <th>Example Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Levenshtein Distance; Levenshtein Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author</td>\n",
       "      <td>Author</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Publication</td>\n",
       "      <td>Publication</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Format</td>\n",
       "      <td>Format</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ISBN</td>\n",
       "      <td>ISBN</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>Exact Match; Absolute Norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Series</td>\n",
       "      <td>Series</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical Details</td>\n",
       "      <td>Physical Details</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Left Attribute   Right Attribute                  Left Attribute Type  \\\n",
       "0                ID                ID                short string (1 word)   \n",
       "1             Title             Title                short string (1 word)   \n",
       "2            Author            Author  medium string (5 words to 10 words)   \n",
       "3       Publication       Publication  medium string (5 words to 10 words)   \n",
       "4            Format            Format     short string (1 word to 5 words)   \n",
       "5              ISBN              ISBN                              numeric   \n",
       "6            Series            Series  medium string (5 words to 10 words)   \n",
       "7  Physical Details  Physical Details                short string (1 word)   \n",
       "\n",
       "                  Right Attribute Type  \\\n",
       "0                short string (1 word)   \n",
       "1                short string (1 word)   \n",
       "2     short string (1 word to 5 words)   \n",
       "3     short string (1 word to 5 words)   \n",
       "4     short string (1 word to 5 words)   \n",
       "5                              numeric   \n",
       "6  medium string (5 words to 10 words)   \n",
       "7     short string (1 word to 5 words)   \n",
       "\n",
       "                                                                              Example Features  \n",
       "0                                                 Levenshtein Distance; Levenshtein Similarity  \n",
       "1  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "2                                                           Not Applicable: Types do not match  \n",
       "3                                                           Not Applicable: Types do not match  \n",
       "4  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "5                                                                   Exact Match; Absolute Norm  \n",
       "6  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "7                                                           Not Applicable: Types do not match  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to proceed? (y/n):y\n"
     ]
    }
   ],
   "source": [
    "match_f = em.get_features_for_matching(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating feature\n",
    "In the feature engineering process, we first exclude the features that are automatically generated by ID and ISBN, and add Jaccard scores on Book Title, Author, Publication and Series with space as the delimiter.  We fit 19 features into Decision Tree, Random Forest, SVM, Naive Bayes, Logistic Regression, Linear Regression with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_t = em.get_tokenizers_for_matching()\n",
    "match_s = em.get_sim_funs_for_matching()\n",
    "f1 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Title\"]), dlm_dc0(rtuple[\"Title\"]))', match_t, match_s)\n",
    "f2 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Author\"]), dlm_dc0(rtuple[\"Author\"]))', match_t, match_s)\n",
    "f3 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Publication\"]), dlm_dc0(rtuple[\"Publication\"]))', match_t, match_s)\n",
    "f4 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Series\"]), dlm_dc0(rtuple[\"Series\"]))', match_t, match_s)\n",
    "em.add_feature(match_f, 'Title_Title_jac_dlm_dc0_dlm_dc0', f1)\n",
    "em.add_feature(match_f, 'Author_Author_jac_dlm_dc0_dlm_dc0', f2)\n",
    "em.add_feature(match_f, 'Publication_Publication_jac_dlm_dc0_dlm_dc0', f3)\n",
    "em.add_feature(match_f, 'Series_Series_jac_dlm_dc0_dlm_dc0', f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debugging\n",
    "After first round of debug, we found that the learning method had difficulty in distinguishing books/journals in different versions/edition/conference. So we introduced a new feature that capture this piece of information embedded in book titles. Basically, we extracted the roman numerals from the title and use 1-0 to indicate whether they are the same from each tuple pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add blackbox feature\n",
    "\n",
    "import re\n",
    "# for Roman numerals matching\n",
    "def Title_Title_blackbox_1(x, y):\n",
    "    \n",
    "    # get name attribute\n",
    "    x_title = x['Title']\n",
    "    y_title = y['Title']\n",
    "    regex_roman = '\\s+[MDCLXVI]+\\s+'\n",
    "    x_match = None\n",
    "    y_match = None\n",
    "    if re.search(regex_roman, x_title):\n",
    "        x_match = re.search(regex_roman, x_title).group(0)\n",
    "    if re.search(regex_roman, y_title):\n",
    "        y_match = re.search(regex_roman, y_title).group(0)\n",
    "\n",
    "    if x_match is None or y_match is None:\n",
    "        return False\n",
    "    else:\n",
    "        return x_match == y_match\n",
    "\n",
    "em.add_blackbox_feature(match_f, 'blackbox_1', Title_Title_blackbox_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we delete features that are related to ID and ISBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f = match_f[(match_f['left_attribute'] != 'ID') & (match_f['left_attribute'] != 'ISBN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature from set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "H = em.extract_feature_vecs(I, feature_table=match_f, attrs_after=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = em.DTMatcher(name='DecisionTree', random_state = 0, max_depth = 5)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name = 'NaiveBayes')\n",
    "\n",
    "result = em.select_matcher(matchers=[dt, rf, svm, lg, ln, nb], \n",
    "                           table=H, \n",
    "                           exclude_attrs=['_id', 'ltable_ID', 'rtable_ID'], \n",
    "                           target_attr='label', \n",
    "                           k=5,\n",
    "                           metric_to_select_matcher='precision'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Average recall</th>\n",
       "      <th>Average f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.886450</td>\n",
       "      <td>0.882262</td>\n",
       "      <td>0.884683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.921318</td>\n",
       "      <td>0.895359</td>\n",
       "      <td>0.880092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.829167</td>\n",
       "      <td>0.664124</td>\n",
       "      <td>0.731630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.870488</td>\n",
       "      <td>0.822088</td>\n",
       "      <td>0.828911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>0.909921</td>\n",
       "      <td>0.925595</td>\n",
       "      <td>0.920403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.779401</td>\n",
       "      <td>0.869124</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Matcher  Average precision  Average recall  Average f1\n",
       "0  DecisionTree           0.886450        0.882262    0.884683\n",
       "1            RF           0.921318        0.895359    0.880092\n",
       "2           SVM           0.829167        0.664124    0.731630\n",
       "3        LogReg           0.870488        0.822088    0.828911\n",
       "4        LinReg           0.909921        0.925595    0.920403\n",
       "5    NaiveBayes           0.779401        0.869124    0.807932"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 30.0),\n",
       "             ('prec_denominator', 32.0),\n",
       "             ('precision', 0.9375),\n",
       "             ('recall_numerator', 30.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.967741935483871),\n",
       "             ('f1', 0.9523809523809523),\n",
       "             ('pred_pos_num', 32.0),\n",
       "             ('false_pos_num', 2.0),\n",
       "             ('false_pos_ls', [('a5426', 'b3056'), ('a3061', 'b99')]),\n",
       "             ('pred_neg_num', 70.0),\n",
       "             ('false_neg_num', 1.0),\n",
       "             ('false_neg_ls', [('a3595', 'b826')])])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "rf.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = rf.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=True, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 25.0),\n",
       "             ('prec_denominator', 27.0),\n",
       "             ('precision', 0.9259259259259259),\n",
       "             ('recall_numerator', 25.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.8064516129032258),\n",
       "             ('f1', 0.8620689655172414),\n",
       "             ('pred_pos_num', 27.0),\n",
       "             ('false_pos_num', 2.0),\n",
       "             ('false_pos_ls', [('a5146', 'b695'), ('a3709', 'b907')]),\n",
       "             ('pred_neg_num', 75.0),\n",
       "             ('false_neg_num', 6.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a4779', 'b2500'),\n",
       "               ('a2488', 'b767'),\n",
       "               ('a1614', 'b3909'),\n",
       "               ('a3595', 'b826'),\n",
       "               ('a2082', 'b2692'),\n",
       "               ('a5196', 'b5855')])])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DT\n",
    "\n",
    "dt.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = dt.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=True, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 21.0),\n",
       "             ('prec_denominator', 31.0),\n",
       "             ('precision', 0.6774193548387096),\n",
       "             ('recall_numerator', 21.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.6774193548387096),\n",
       "             ('f1', 0.6774193548387096),\n",
       "             ('pred_pos_num', 31.0),\n",
       "             ('false_pos_num', 10.0),\n",
       "             ('false_pos_ls',\n",
       "              [('a4814', 'b140'),\n",
       "               ('a5426', 'b3056'),\n",
       "               ('a3061', 'b99'),\n",
       "               ('a5426', 'b592'),\n",
       "               ('a5640', 'b685'),\n",
       "               ('a188', 'b4616'),\n",
       "               ('a5426', 'b695'),\n",
       "               ('a2905', 'b1241'),\n",
       "               ('a4984', 'b1911'),\n",
       "               ('a5426', 'b685')]),\n",
       "             ('pred_neg_num', 71.0),\n",
       "             ('false_neg_num', 10.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a420', 'b5252'),\n",
       "               ('a4451', 'b928'),\n",
       "               ('a3594', 'b858'),\n",
       "               ('a120', 'b4255'),\n",
       "               ('a1876', 'b862'),\n",
       "               ('a254', 'b3231'),\n",
       "               ('a5393', 'b2293'),\n",
       "               ('a651', 'b385'),\n",
       "               ('a3595', 'b826'),\n",
       "               ('a1578', 'b227')])])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "svm.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = svm.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=False, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 27.0),\n",
       "             ('prec_denominator', 32.0),\n",
       "             ('precision', 0.84375),\n",
       "             ('recall_numerator', 27.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.8709677419354839),\n",
       "             ('f1', 0.8571428571428571),\n",
       "             ('pred_pos_num', 32.0),\n",
       "             ('false_pos_num', 5.0),\n",
       "             ('false_pos_ls',\n",
       "              [('a3709', 'b907'),\n",
       "               ('a4969', 'b907'),\n",
       "               ('a5146', 'b695'),\n",
       "               ('a5426', 'b3056'),\n",
       "               ('a625', 'b685')]),\n",
       "             ('pred_neg_num', 70.0),\n",
       "             ('false_neg_num', 4.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a4779', 'b2500'),\n",
       "               ('a3595', 'b826'),\n",
       "               ('a1876', 'b862'),\n",
       "               ('a3020', 'b4720')])])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR\n",
    "\n",
    "lg.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = lg.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=False, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature from set J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 28.0),\n",
       "             ('prec_denominator', 31.0),\n",
       "             ('precision', 0.9032258064516129),\n",
       "             ('recall_numerator', 28.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.9032258064516129),\n",
       "             ('f1', 0.9032258064516129),\n",
       "             ('pred_pos_num', 31.0),\n",
       "             ('false_pos_num', 3.0),\n",
       "             ('false_pos_ls',\n",
       "              [('a5146', 'b695'), ('a2493', 'b928'), ('a2714', 'b2276')]),\n",
       "             ('pred_neg_num', 71.0),\n",
       "             ('false_neg_num', 3.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a4779', 'b2500'), ('a3595', 'b826'), ('a1876', 'b862')])])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LN\n",
    "\n",
    "ln.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = ln.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=False, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 27.0),\n",
       "             ('prec_denominator', 40.0),\n",
       "             ('precision', 0.675),\n",
       "             ('recall_numerator', 27.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.8709677419354839),\n",
       "             ('f1', 0.7605633802816901),\n",
       "             ('pred_pos_num', 40.0),\n",
       "             ('false_pos_num', 13.0),\n",
       "             ('false_pos_ls',\n",
       "              [('a5399', 'b685'),\n",
       "               ('a4324', 'b2244'),\n",
       "               ('a3709', 'b907'),\n",
       "               ('a4969', 'b907'),\n",
       "               ('a5146', 'b695'),\n",
       "               ('a5426', 'b3056'),\n",
       "               ('a2493', 'b928'),\n",
       "               ('a3061', 'b99'),\n",
       "               ('a5640', 'b685'),\n",
       "               ('a5640', 'b907'),\n",
       "               ('a625', 'b685'),\n",
       "               ('a105', 'b2244'),\n",
       "               ('a1638', 'b685')]),\n",
       "             ('pred_neg_num', 62.0),\n",
       "             ('false_neg_num', 4.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a4779', 'b2500'),\n",
       "               ('a420', 'b5252'),\n",
       "               ('a1876', 'b862'),\n",
       "               ('a3020', 'b4720')])])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB\n",
    "\n",
    "nb.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')\n",
    "\n",
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])\n",
    "pred_table = nb.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=False, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)\n",
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')\n",
    "eval_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
