{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching (EM) about Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This IPython notebook shows a basic workflow two tables using *py_entitymatching*. We want to match data science books in library of UW-Madison and UIUC.  The book information of UW-Madison is from [here](https://search.library.wisc.edu/search/system?q=Data+Science) and the book information of UIUC is from [here](https://vufind.carli.illinois.edu/vf-uiu/Search/Home?lookfor=Data+Science+&type=all&start_over=1&submit=Find&search=new). Details can be found from our Stage 2 Report [here](https://github.com/iphyer/CS839ClassProject/blob/master/stage2/Stage2Report.pdf). \n",
    "\n",
    "\n",
    "First, we need to import *py_entitymatching* package and other libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import py_entitymatching as em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input tables\n",
    "\n",
    "We begin by loading the input tables.\n",
    "\n",
    "We name the table about UW-Madison `TableA.csv` and the table about UIUC `TableB.csv`. And there are \n",
    "\n",
    "* 4824 tuples in table `TableA.csv`\n",
    "* 5060 tuples in table `TableB.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata file is not present in the given path; proceeding to read the csv file.\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "table_A = em.read_csv_metadata('../data/TableA.csv', key = 'ID')\n",
    "table_B = em.read_csv_metadata('../data/TableB.csv', key = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4824, 8)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5060, 8)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down sampling\n",
    "Down sampling table A and Bï¼Œ get 1000 examples from both table A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = em.down_sample(table_A, table_B, size=1000, y_param = 1, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_f = em.get_features_for_blocking(A, B)\n",
    "block_t = em.get_tokenizers_for_blocking()\n",
    "block_s = em.get_sim_funs_for_blocking()\n",
    "r = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Title\"]), dlm_dc0(rtuple[\"Title\"]))', block_t, block_s)\n",
    "em.add_feature(block_f, 'Title_Title_jac_dlm_dc0_dlm_dc0', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block tables to get candidate set\n",
    "\n",
    "Here we will use several blockers to remove obviously non-matching tuple pairs from the input tables.\n",
    "\n",
    "For the same book, since we got the data from two different library websites, their attributes may not be the exact same. Therefore, we applied an OverlapBlocker over some of the attributes, including the *Title* and *Author*.\n",
    "\n",
    "After multiple tests, we found the best overlap_size for each attribute - for *Author* and *Title*, we set the overlap_size to be 2 and 4 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = em.OverlapBlocker()\n",
    "C = ob.block_tables(A, B, 'Author', 'Author', \n",
    "                    l_output_attrs=['Title','Author','Publication','Format','ISBN','Series', 'Physical Details'], \n",
    "                    r_output_attrs=['Title','Author','Publication','Format','ISBN','Series', 'Physical Details'], \n",
    "                    overlap_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ob.block_candset(C, 'Title', 'Title', overlap_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D[['ltable_Title', 'rtable_Title']].to_csv('test1.csv', sep = ',')\n",
    "# E = ob.block_candset(D, 'Series', 'Series', overlap_size = 1)\n",
    "# F = ob.block_candset(E, 'Publication', 'Publication', overlap_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule1 = ['Title_Title_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) < 0.3']\n",
    "# rb = em.RuleBasedBlocker()\n",
    "# rb.add_rule(rule1, block_f)\n",
    "# G = rb.block_candset(D)\n",
    "# G[['ltable_Title', 'rtable_Title']].to_csv('test.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of blocking\n",
    "Set D contains all examples after blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D[D['ltable_ISBN'] == D['rtable_ISBN']].shape\n",
    "D.to_csv('Set_C.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E = em.label_table(D, label_column_name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from D\n",
    "Sample 300 examples from D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = em.sample_table(E, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S.to_csv('Set_S.csv', sep = ',')\n",
    "# em.to_csv_metadata(S, './table_S.csv')\n",
    "# S[S['ltable_ISBN'] == S['rtable_ISBN']].shape\n",
    "# S[\"label\"] = (S[\"ltable_ISBN\"] == S[\"rtable_ISBN\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create label\n",
    "After manually labeling the data, We get 300 candidates with labels in label_S. <br/>\n",
    "Also, need to set the metadata for label_S appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_S = pd.read_csv('./data_with_label.csv')\n",
    "# em.copy_properties(S, label_S)\n",
    "em.set_property(label_S, 'key', '_id')\n",
    "em.set_property(label_S, 'fk_ltable', 'ltable_ID')\n",
    "em.set_property(label_S, 'fk_rtable', 'rtable_ID')\n",
    "label_S_rtable = em.read_csv_metadata('./label_S_rtable.csv')\n",
    "label_S_ltable = em.read_csv_metadata('./label_S_ltable.csv')\n",
    "em.set_property(label_S, 'rtable', label_S_rtable)\n",
    "em.set_property(label_S, 'ltable', label_S_ltable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(em.get_property(I, 'key'))\n",
    "# print(em.get_property(I, 'ltable'))\n",
    "# print(em.get_property(I, 'rtable'))\n",
    "# I_rtable = em.get_property(I, 'rtable')\n",
    "# em.set_property(label_S, 'rtable', I_rtable)\n",
    "# I_ltable = em.get_property(I, 'ltable')\n",
    "# em.set_property(label_S, 'ltable', I_ltable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em.to_csv_metadata(I_rtable, './label_S_rtable.csv')\n",
    "# em.to_csv_metadata(I_ltable, './label_S_ltable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_S = em.read_csv_metadata('./label_S')\n",
    "# em.to_csv_metadata(label_S, './label_S.csv')\n",
    "# label_S_new = em.read_csv_metadata('./label_S.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = label_S[['_id','label']]\n",
    "# df.columns=['lid','label']\n",
    "# S_new = S.iloc[:,:-1]\n",
    "# S_new = pd.concat([S_new, df], axis = 1, ignore_index = False)\n",
    "# S_new = S_new.merge(df,left_on='_id',right_on='lid', how = 'inner')\n",
    "# S_new = S_new.drop(columns=['lid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em.get_key(S)\n",
    "# em.set_key(S_new, '_id')\n",
    "# em.set_fk_ltable(S_new, 'ltable_ID')\n",
    "# em.set_fk_rtable(S_new, 'rtable_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_S[label_S['label'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em.get_fk_rtable(S_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S.to_csv('Set_G.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "IJ = em.split_train_test(label_S, train_proportion=0.66, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.to_csv('Set_I.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J.to_csv('Set_J.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_f = em.get_features_for_blocking(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table shows the corresponding attributes along with their respective types.\n",
      "Please confirm that the information  has been correctly inferred.\n",
      "If you would like to skip this validation process in the future,\n",
      "please set the flag validate_inferred_attr_types equal to false.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left Attribute</th>\n",
       "      <th>Right Attribute</th>\n",
       "      <th>Left Attribute Type</th>\n",
       "      <th>Right Attribute Type</th>\n",
       "      <th>Example Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>ID</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Levenshtein Distance; Levenshtein Similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Author</td>\n",
       "      <td>Author</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Publication</td>\n",
       "      <td>Publication</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Format</td>\n",
       "      <td>Format</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ISBN</td>\n",
       "      <td>ISBN</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>Exact Match; Absolute Norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Series</td>\n",
       "      <td>Series</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>medium string (5 words to 10 words)</td>\n",
       "      <td>Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Physical Details</td>\n",
       "      <td>Physical Details</td>\n",
       "      <td>short string (1 word)</td>\n",
       "      <td>short string (1 word to 5 words)</td>\n",
       "      <td>Not Applicable: Types do not match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Left Attribute   Right Attribute                  Left Attribute Type  \\\n",
       "0                ID                ID                short string (1 word)   \n",
       "1             Title             Title                short string (1 word)   \n",
       "2            Author            Author  medium string (5 words to 10 words)   \n",
       "3       Publication       Publication  medium string (5 words to 10 words)   \n",
       "4            Format            Format     short string (1 word to 5 words)   \n",
       "5              ISBN              ISBN                              numeric   \n",
       "6            Series            Series  medium string (5 words to 10 words)   \n",
       "7  Physical Details  Physical Details                short string (1 word)   \n",
       "\n",
       "                  Right Attribute Type  \\\n",
       "0                short string (1 word)   \n",
       "1                short string (1 word)   \n",
       "2     short string (1 word to 5 words)   \n",
       "3     short string (1 word to 5 words)   \n",
       "4     short string (1 word to 5 words)   \n",
       "5                              numeric   \n",
       "6  medium string (5 words to 10 words)   \n",
       "7     short string (1 word to 5 words)   \n",
       "\n",
       "                                                                              Example Features  \n",
       "0                                                 Levenshtein Distance; Levenshtein Similarity  \n",
       "1  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "2                                                           Not Applicable: Types do not match  \n",
       "3                                                           Not Applicable: Types do not match  \n",
       "4  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "5                                                                   Exact Match; Absolute Norm  \n",
       "6  Jaccard Similarity [3-grams, 3-grams]; Cosine Similarity [Space Delimiter, Space Delimiter]  \n",
       "7                                                           Not Applicable: Types do not match  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to proceed? (y/n):y\n"
     ]
    }
   ],
   "source": [
    "match_f = em.get_features_for_matching(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_t = em.get_tokenizers_for_matching()\n",
    "match_s = em.get_sim_funs_for_matching()\n",
    "f1 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Title\"]), dlm_dc0(rtuple[\"Title\"]))', match_t, match_s)\n",
    "f2 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Author\"]), dlm_dc0(rtuple[\"Author\"]))', match_t, match_s)\n",
    "f3 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Publication\"]), dlm_dc0(rtuple[\"Publication\"]))', match_t, match_s)\n",
    "f4 = em.get_feature_fn('jaccard(dlm_dc0(ltuple[\"Series\"]), dlm_dc0(rtuple[\"Series\"]))', match_t, match_s)\n",
    "em.add_feature(match_f, 'Title_Title_jac_dlm_dc0_dlm_dc0', f1)\n",
    "em.add_feature(match_f, 'Author_Author_jac_dlm_dc0_dlm_dc0', f2)\n",
    "em.add_feature(match_f, 'Publication_Publication_jac_dlm_dc0_dlm_dc0', f3)\n",
    "em.add_feature(match_f, 'Series_Series_jac_dlm_dc0_dlm_dc0', f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-192-99c38684fca7>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-192-99c38684fca7>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    for number matching (e.g. 6th edition)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Add blackbox feature\n",
    "\n",
    "import re\n",
    "# for Roman numerals matching\n",
    "def Title_Title_blackbox_1(x, y):\n",
    "    \n",
    "    # get name attribute\n",
    "    x_title = x['Title']\n",
    "    y_title = y['Title']\n",
    "    regex_roman = '\\s+[MDCLXVI]+\\s+'\n",
    "    x_match = None\n",
    "    y_match = None\n",
    "    if re.search(regex_roman, x_title):\n",
    "        x_match = re.search(regex_roman, x_title).group(0)\n",
    "    if re.search(regex_roman, y_title):\n",
    "        y_match = re.search(regex_roman, y_title).group(0)\n",
    "\n",
    "    if x_match is None or y_match is None:\n",
    "        return False\n",
    "    else:\n",
    "        return x_match == y_match\n",
    "\n",
    "em.add_blackbox_feature(match_f, 'blackbox_1', Title_Title_blackbox_1)\n",
    "\n",
    "\n",
    "for number matching (e.g. 6th edition)\n",
    "def Title_Title_blackbox_2(x, y):\n",
    "    # x, y will be of type pandas series\n",
    "    \n",
    "    x_title = x['Title']\n",
    "    y_title = y['Title']\n",
    "    regex_number = '\\s+(\\d+)\\s*th'\n",
    "    x_match = None\n",
    "    y_match = None\n",
    "    if re.search(regex_number, x_title):\n",
    "        x_match = re.search(regex_number, x_title).group(1)\n",
    "    if re.search(regex_number, y_title):\n",
    "        y_match = re.search(regex_number, y_title).group(1)\n",
    "\n",
    "    if x_match is None or y_match is None:\n",
    "        return False\n",
    "    else:\n",
    "        return x_match == y_match\n",
    "\n",
    "em.add_blackbox_feature(match_f, 'blackbox_2', Title_Title_blackbox_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# regex_roman = '\\s+(\\d+)\\s*th'\n",
    "\n",
    "# pattern = re.compile(regex_roman)\n",
    "# x_title = '\"Public key cryptography--PKC 2004 : 7th International Workshop on Theory and Practice in Public...'\n",
    "# pattern.search(x_title).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we delete features that are related to ID and ISBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f = match_f[(match_f['left_attribute'] != 'ID') & (match_f['left_attribute'] != 'ISBN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature from set I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "H = em.extract_feature_vecs(I, feature_table=match_f, attrs_after=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H['blackbox_2'].sum()\n",
    "# H[(H['blackbox_2'] == True) & (H['label'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = em.DTMatcher(name='DecisionTree', random_state = 0, max_depth = 5)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher('NaiveBayes')\n",
    "\n",
    "result = em.select_matcher(matchers=[dt, rf, svm, lg, ln], \n",
    "                           table=H, \n",
    "                           exclude_attrs=['_id', 'ltable_ID', 'rtable_ID'], \n",
    "                           target_attr='label', \n",
    "                           k=5,\n",
    "                           metric_to_select_matcher='precision'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Average precision</th>\n",
       "      <th>Average recall</th>\n",
       "      <th>Average f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.845203</td>\n",
       "      <td>0.908250</td>\n",
       "      <td>0.846377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.889492</td>\n",
       "      <td>0.895561</td>\n",
       "      <td>0.923153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.775531</td>\n",
       "      <td>0.688355</td>\n",
       "      <td>0.739918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.844767</td>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.838140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>0.935833</td>\n",
       "      <td>0.913828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Matcher  Average precision  Average recall  Average f1\n",
       "0  DecisionTree           0.845203        0.908250    0.846377\n",
       "1            RF           0.889492        0.895561    0.923153\n",
       "2           SVM           0.775531        0.688355    0.739918\n",
       "3        LogReg           0.844767        0.841471    0.838140\n",
       "4        LinReg           0.906800        0.935833    0.913828"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(table=H, \n",
    "       exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "       target_attr='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "H_test = em.extract_feature_vecs(J, feature_table=match_f, attrs_after=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature from set J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_table = rf.predict(table= H_test, \n",
    "                        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "                        target_attr='predicted_labels', \n",
    "                        return_probs=True, \n",
    "                        probs_attr='proba', \n",
    "                        append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ln.fit(table=H, \n",
    "#        exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "#        target_attr='label')\n",
    "# pred_table = ln.predict(table= H_test, \n",
    "#                         exclude_attrs=['_id', 'ltable_ID', 'rtable_ID', 'label'], \n",
    "#                         target_attr='predicted_labels', \n",
    "#                         return_probs=True, \n",
    "#                         probs_attr='proba', \n",
    "#                         append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_table[(pred_table['label'] == 1) & (pred_table['predicted_labels'] == 0)]\n",
    "# wrong = pred_table[(pred_table['label'] != pred_table['predicted_labels'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# J\n",
    "# J.join(wrong, on = '_id', lsuffix = '_id', rsuffix = '_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(J[J['_id'] == 358]['ltable_Title'])\n",
    "# print(J[J['_id'] == 358]['rtable_Title'])\n",
    "# print(J[J['_id'] == 332]['ltable_Title'])\n",
    "# print(J[J['_id'] == 332]['rtable_Title'])\n",
    "# print(J[J['_id'] == 509]['ltable_Title'])\n",
    "# print(J[J['_id'] == 509]['rtable_Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary = em.eval_matches(pred_table, 'label', 'predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('prec_numerator', 27.0),\n",
       "             ('prec_denominator', 30.0),\n",
       "             ('precision', 0.9),\n",
       "             ('recall_numerator', 27.0),\n",
       "             ('recall_denominator', 31.0),\n",
       "             ('recall', 0.8709677419354839),\n",
       "             ('f1', 0.8852459016393444),\n",
       "             ('pred_pos_num', 30.0),\n",
       "             ('false_pos_num', 3.0),\n",
       "             ('false_pos_ls',\n",
       "              [('a5146', 'b695'), ('a3061', 'b99'), ('a3709', 'b907')]),\n",
       "             ('pred_neg_num', 72.0),\n",
       "             ('false_neg_num', 4.0),\n",
       "             ('false_neg_ls',\n",
       "              [('a4779', 'b2500'),\n",
       "               ('a3595', 'b826'),\n",
       "               ('a120', 'b4255'),\n",
       "               ('a1876', 'b862')])])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
